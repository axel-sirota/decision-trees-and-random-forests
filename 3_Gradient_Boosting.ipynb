{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axel-sirota/decision-trees-and-random-forests/blob/main/3_Gradient_Boosting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFd3ULbeyZZp"
      },
      "source": [
        "# Gradient Boosting\n",
        "\n",
        "Today I’m going to walk you through training a simple classification model. Although scikit-learn and other packages contain simpler models with few parameters (SVM comes to mind), gradient boosted trees are shown to be very powerful classifiers in a wide variety of datasets and problems.\n",
        "\n",
        "XGBoost is one of the most popular libraries used to pursue classification and regression using machine learning, but without resorting to deep learning techniques, such as neural networks trained in Keras or PyTorch, for example. If you started your journey into data science by comparing different types of regressions or Naive Bayes classifiers, XGBoost is a wonderful tool to produce more accurate, robust models than even the well-performing RandomForestClassifier found in scikit-learn.\n",
        "\n",
        "\n",
        "## Starting with data:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost"
      ],
      "metadata": {
        "id": "bmqY6GJkEwOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Qmf-Jeo_nsH"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import time\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, average_precision_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "import warnings  # `do not disturbe` mode\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnaqoCZ6aipc"
      },
      "source": [
        "If you’re using virtualenv or another Python environment management system, feel free to do a pip install instead, or simply insert your own preferred method of installation.\n",
        "\n",
        "## Setting a baseline:\n",
        "\n",
        "We’re importing a RandomForestClassifier as a baseline to compare against, and using datasets and metrics functions from scikit-learn. Let’s go ahead and import the sample dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXfEhRfDagf_"
      },
      "source": [
        "bc_dataset = load_breast_cancer()\n",
        "bc_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = pd.DataFrame(bc_dataset.data)\n",
        "X.columns = bc_dataset.feature_names\n",
        "y = bc_dataset.target\n"
      ],
      "metadata": {
        "id": "nt64itXEGD8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "id": "oDd9t37kGZmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "LY_G_kMDGfbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(y).value_counts()"
      ],
      "metadata": {
        "id": "RRe0JHWuGqzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split data into three parts: 80% train, 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "8C2aCumqGIdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nq77qtt1pc4"
      },
      "source": [
        "With a result of around 0.6, you can see that this dataset isn’t terribly imbalanced. How convenient! Still, we can pass this value into our model to achieve greater accuracy, which we’ll do below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KioUN-E3tZk"
      },
      "source": [
        "### An aside, looking at a Random Forest Classifier to compare to a simpler model:\n",
        "\n",
        "Just so that we reassure ourselves that a simpler and less computationally intensive model won't serve us even better, let's try out a Random Forest Classifier from scikit-learn:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLH6BlZh4A0d"
      },
      "source": [
        "clf = RandomForestClassifier(max_depth=2, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "accuracy_score(y_test, clf.predict(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = (y == 0).sum() / (1.0 * (y == 1).sum())\n",
        "weights"
      ],
      "metadata": {
        "id": "nZfktrBoHT6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nbDFNOs_7_e"
      },
      "source": [
        "model = xgb.XGBClassifier(\n",
        "                          scale_pos_weight = weights,\n",
        "                          n_jobs = 4,\n",
        "                          objective='binary:logistic',\n",
        "                          use_label_encoder = False\n",
        "                        )\n",
        "\n",
        "start = time.time()\n",
        "model.fit(X_train, y_train, eval_metric='logloss')\n",
        "fittingTime = time.time() - start\n",
        "\n",
        "start = time.time()\n",
        "prediction = model.predict(X_test)\n",
        "InferenceTime = time.time() - start\n",
        "\n",
        "F1score = f1_score(y_test, prediction)\n",
        "probabilities = model.predict_proba(X_test)\n",
        "AUPRC = average_precision_score(y_test, probabilities[:, 1])\n",
        "acc = accuracy_score(y_test, model.predict(X_test))\n",
        "\n",
        "print('AUPRC = {}'.format(average_precision_score(y_test, probabilities[:, 1])))\n",
        "print('F1 Score = {}'.format(F1score))\n",
        "print('Fitting Time = {}'.format(fittingTime))\n",
        "print('Inference Time = {}'.format(InferenceTime))\n",
        "print('Accuracy = {}'.format(acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cKdcIDH1xk3"
      },
      "source": [
        "As you can see, we now have multiple metrics that we can track: AUPRC, F1, fitting time, and inference time. Depending on your use case, these values may make or break your model in production.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4mKYER05U1y"
      },
      "source": [
        "\n",
        "### Background on XGBoost’s parameters:\n",
        "\n",
        "* `min_child_weight`, used to control over-fitting, this parameter is the sample size under which the\n",
        "model can not split a node. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
        "* `max_depth`, this is the maximum depth of a tree. This parameter controls over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n",
        "* `gamma`, this parameter specifies the minimum loss reduction required to make a split. The larger gamma is, the more conservative the algorithm will be.\n",
        "subsample, defines the ratio of the training instances. For example setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees, preventing overfitting\n",
        "* `colsample_bytree`, defines the fraction of features to be randomly sampled for each tree.\n",
        "alpha and lambda are the L1 and L2 regularization terms on weights. Both values tend to prevent overfitting as they are increased. Additionally, Alpha can be used in case of very high dimensionality to help a model training converge faster\n",
        "* `learning_rate` controls the weighting of new trees added to the model. Lowering this value will prevent overfitting, but require the model to add a larger number of tree\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now you do it\n",
        "<img src=\"https://www.dropbox.com/scl/fi/s9kv1dytq4qzr8g19y3r0/hands_on.jpg?rlkey=yz8kq22sfdgc7lsgmm1e0fksr&raw=1\" width=\"100\" height=\"100\" align=\"right\"/>\n",
        "\n",
        "\n",
        "You will predict the position of NBA players based on their statistics"
      ],
      "metadata": {
        "id": "cWQ4urJpH7ka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile get_data.sh\n",
        "mkdir -p ./data\n",
        "if [ ! -f data/NBA_players_2015.csv ]; then\n",
        "  wget -O data/NBA_players_2015.csv https://www.dropbox.com/scl/fi/0jgo8u5lbphvwwl2btq1w/NBA_players_2015.csv?rlkey=q86m5lp3ycndh5jbegvjewwzu&dl=0\n",
        "fi"
      ],
      "metadata": {
        "id": "di47faWfID9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!bash get_data.sh"
      ],
      "metadata": {
        "id": "2ulfRwm8II7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use these parameters for testing\n",
        "\n",
        "> random_state = 99\n",
        "\n",
        "> test_size = 0.2"
      ],
      "metadata": {
        "id": "x7k9DrBrIA0w"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXkHFyMv5zv2"
      },
      "source": [
        "nba = pd.read_csv('./data/NBA_players_2015.csv')\n",
        "nba.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7VlN_YyHIQIE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}